<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="description"
      content="Online Learning of Deceptive Policies under Intermittent Observation"
    />
    <meta name="keywords" content="Keywords" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      Online Learning of Deceptive Policies under Intermittent Observation
    </title>
    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css"
    />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']]
        }
      };
    </script>
    <style>
      .question-section {
        margin: 2rem 0;
        padding: 2rem;
        background-color: #f5f5f5;
        border-radius: 8px;
      }
      .image-comparison {
        display: flex;
        gap: 2rem;
        margin-top: 2rem;
      }
      .image-container {
        flex: 1;
        text-align: center;
      }
      .image-container img {
        max-width: 100%;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
      }
      .question-type {
        margin-bottom: 1rem;
      }
      .author-block {
        margin-right: 5px;
      }
      .author-equal {
        font-size: 0.85em;
        vertical-align: super;
      }
      .institution-logos {
        display: flex;
        justify-content: center;
        align-items: center;
        gap: 2rem;
        margin-top: 2rem;
      }
      .institution-logo {
        max-height: 80px;
        width: auto;
      }
    </style>
  </head>
  <body>
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                Online Learning of Deceptive Policies under Intermittent Observation
              </h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  Anonymous
                </span>
              </div>
              
              
              
              
              <div class="column has-text-centered mt-4">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <span class="link-block">
                    <a
                      href="paper.pdf"
                      class="button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <!-- Code Link. -->
                  <span class="link-block">
                    <a
                      href="https://github.com/tom-deception/tom-deception.github.io"
                      class="button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
  </body>
</html>

  <!-- Abstract Section -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Abstract</h2>
          <div class="content has-text-justified">
            <div style="display: flex; align-items: flex-start; gap: 2rem; margin-bottom: 1rem;">
              <div style="flex: 1;">
                <p>
                  In supervisory control settings, autonomous systems are not monitored continuously. Instead, monitoring often occurs at sporadic intervals within known bounds. We study the problem of deception, where an agent pursues a private objective while remaining plausibly compliant with a supervisor's reference policy when observations occur.
                  Motivated by the behavior of real, human supervisors, we situate the problem within Theory of Mind: the representation of what an observer believes and expects to see.
                  We show that Theory of Mind can be repurposed to steer online reinforcement learning (RL) toward such deceptive behavior. We model the supervisor's expectations and distill from them a single, calibrated scalar --- the expected evidence of deviation if an observation were to happen now. This scalar combines how unlike the reference and current action distributions appear, with the agent's belief that an observation is imminent. Injected as a state-dependent weight into a KL-regularized policy improvement step within an online RL loop, this scalar informs a closed-form update that smoothly trades off self-interest and compliance, thus sidestepping hand-crafted or heuristic policies. 
                  In real-world, real-time hardware experiments on marine (ASV) and aerial (UAV) navigation, our ToM-guided RL runs online, achieves high return and success with observed-trace evidence calibrated to the supervisor's expectations.
                </p>
              </div>
              <div style="flex: 0 0 300px; text-align: center;">
                <img src="assets/teaseri.svg" alt="Teaser Figure" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);" />
                <p style="font-size: 0.9em; color: #666; margin-top: 0.5rem; text-align: left;">
                  <em>Illustration of deception under intermittent observation. An observer assigns a task and specifies a reference behavior; the agent pursues a private objective. The observer sees only sporadic snapshots (state–action pairs). We ask: can we design an online policy that looks compliant when watched yet exploits unobserved intervals without exceeding a detectability budget?</em>
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  </body>
</html>
    </section>

<!-- Problem Formulation Section -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Problem Formulation</h2>
    <div class="content has-text-justified">
      <p>
        Solving deception under intermittent observation is, at its core, a constrained stochastic control problem: maximize private return while keeping what is seen statistically consistent with a reference behavior. 
        We decouple the problem into three parts and recombine them at decision time.
        (i) <em>Observation:</em> model monitoring as a renewal, state-independent process with bounded gaps and maintain a belief over "time since last observation".
        (ii) <em>Detectability:</em> specify how the supervisor would judge a state–action snapshot if observed, giving a per-step measure of "evidence of deviation."
        (iii) <em>Control:</em> modulate actions online by trading off task value against that evidence. The resulting signal must be local in time and light enough to steer on-policy updates.
        Existing approaches either plan jointly over hidden monitoring (quickly intractable), keep the agent globally close to the reference (over-constraining when it is safe to deviate), or adversarially train a detector. 
        What is missing is a calibrated, state- and time-local quantity that answers: "If I were seen now, how incriminating would this action look?"
        Our first instinct would be to model the supervisor's inference end-to-end, i.e., a rich ToM that tracks the supervisor's posterior, path-dependent tests, and how partial observations shape future scrutiny. 
        In practice that is overkill: it is computationally heavy, brittle online, and couples learning to a second adaptive model. 
        Guided by the setup in Sec.~\ref{sec:ps}, we hypothesize that if, at each step, we (i) estimate the probability of being observed under renewal, state-independent monitoring and (ii) measure, at the current state, how far the agent's action distribution departs from the supervisor's reference, then combining these into a single per-step quantity yields a calibrated signal <em>sufficient</em> to drive an online algorithm. To know how this is done, please read our paper.
      </p>
    </div>
  </div>
</section>
<!-- Experimental Videos Section -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Experimental Videos</h2>
    <p class="has-text-justified">
       We evaluate our method in an agent–observer setting where an external observer (e.g., an overhead camera) provides sporadic monitoring consistent with the model and reveals the full state only at observation times. The observer also specifies a reference policy $\pi^S(\cdot\mid s)$ that encodes the behavior expected when the agent is observed, while the agent executes its own task-optimized policy $\pi(\cdot\mid s)$ subject to the detectability budget defined earlier. 
       Between observations the agent receives no information from the observer. 
       A one-step delayed token informs the agent ex post whether time $t$ was observed (it is notified at $t{+}1$ if $O_t{=}1$). The agent has its own private goal, given by the return $J_A(\pi)$, and must keep its average evidential exposure within $\mathcal R_{\mathrm{det}}^{\mathrm{avg}}(\pi)\le \varepsilon$.
    </p>
    
<div style="width:70%; margin: 0 auto;">
  <!-- Video 1 -->
  <div class="box" style="margin-top: 2rem;">
    <h3 class="title is-4 mt-3">Experiment 1</h3>
    <video controls style="width: 100%; border-radius: 8px;">
      <source src="assets/expt1.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
    <p></p>
  </div>
  
  <!-- Video 2 -->
  <div class="box" style="margin-top: 2rem;">
    <h3 class="title is-4 mt-3">Experiment 2</h3>
    <video controls style="width: 100%; border-radius: 8px;">
      <source src="assets/expt2.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
    <p></p>
  </div>
  
  <!-- Video 3 -->
  <div class="box" style="margin-top: 2rem;">
    <h3 class="title is-4 mt-3">Experiment 3</h3>
    <video controls style="width: 100%; border-radius: 8px;">
      <source src="assets/expt3.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
    <p></p>
  </div>
</div>
</section>

<!-- Results Section -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Results</h2>
    <p class="has-text-centered">
      Our experiments reveal five key insights about ToM-conditioned RL performance across different scenarios.
    </p>
    
    <!-- Table 1 -->
    <div class="box" style="margin-top: 2rem; text-align: center;">
      <img src="assets/tab1.png" alt="Table 1: Experimental Results" style="max-width: 100%; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);" />
    </div>
    
    <!-- First row: Key Findings 1 and 2 -->
    <div class="tile is-ancestor">
      <!-- Key Finding 1 -->
      <div class="tile is-parent">
        <div class="tile is-child box">
          <h3 class="title is-4">Key Finding 1</h3>
          <p>
            <strong>ToM-conditioned RL maintains near–task-optimal success while operating at near–always-compliant stealth.</strong>
            Our method achieves high task performance while maintaining low detectability across different observation patterns.
          </p>
          <div class="box" style="text-align: center;">
            <img src="assets/kf1.svg" alt="Key Finding 1" style="max-width: 100%; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);" />
          </div>
        </div>
      </div>
      
      <!-- Key Finding 2 -->
      <div class="tile is-parent">
        <div class="tile is-child box">
          <h3 class="title is-4">Key Finding 2</h3>
          <p>
            <strong>Our method scales seamlessly while remaining low detectability.</strong>
            The approach maintains consistent performance and stealth characteristics across different problem scales and complexities.
          </p>
          <div class="box" style="text-align: center;">
            <img src="assets/kf2.svg" alt="Key Finding 2" style="max-width: 100%; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);" />
          </div>
        </div>
      </div>
    </div>
    
    <!-- Second row: Key Findings 3 and 4&5 -->
    <div class="tile is-ancestor" style="margin-top: 2rem;">
      <!-- Key Finding 3 -->
      <div class="tile is-parent">
        <div class="tile is-child box">
          <h3 class="title is-4">Key Finding 3</h3>
          <p>
            <strong>ToM-conditioned RL exhibits creative maneuvers and accelerates online learning vs baselines.</strong>
            Our approach discovers novel strategies and learns more efficiently compared to standard reinforcement learning methods.
          </p>
          <div class="box" style="text-align: center;">
            <img src="assets/kf3.svg" alt="Key Finding 3" style="max-width: 100%; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);" />
          </div>
        </div>
      </div>
      
      <!-- Key Findings 4 and 5 -->
      <div class="tile is-parent">
        <div class="tile is-child box">
          <h3 class="title is-4">Key Findings 4 & 5</h3>
          <p>
            <strong>ToM scalar is numerically reliable and context-discriminative.</strong>
            The Theory of Mind scalar provides consistent and meaningful signals that effectively distinguish between different contextual situations.
          </p>
          <p>
            <strong>Wider observation gaps are automatically converted into higher reward without retuning.</strong>
            The system naturally adapts to longer periods between observations by increasing reward optimization without requiring manual parameter adjustments.
          </p>
          <div class="box" style="text-align: center;">
            <img src="assets/kf45.svg" alt="Key Findings 4 & 5" style="max-width: 100%; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);" />
          </div>
        </div>
      </div>
    </div> <!-- End of second row tiles -->
    
  </div>
</section>


<!-- Citation Section -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Citation</h2>
    <p class="has-text-centered">
      Please use the following BibTeX entry to cite our work:
    </p>
    <pre style="background-color: #f5f5f5; padding: 1rem; border-radius: 8px; overflow-x: auto;">
@misc{trace2025,
  title = {Online Learning of Deceptive Policies under Intermittent Observation},
  author = {Anonymous},
  year = {2025}
}
    </pre>
  </div>
</section>
</html>
